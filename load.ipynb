{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82067ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "from supabase import create_client, Client\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from itertools import islice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f950fb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3ead679",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"preprocessed_redditData-removedtopic.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "674f1fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started!\n",
      "Unique content rows: 63805\n",
      "Existing content hashes in DB: 0\n",
      "Missing content rows to insert: 63805\n",
      "Total content IDs mapped: 63805\n",
      "Total fact_post rows ready: 63986\n",
      "Inserted 63986 rows into fact_post.\n"
     ]
    }
   ],
   "source": [
    "print(\"Script started!\", flush=True)\n",
    "def hash_content(content: str) -> str:\n",
    "    return hashlib.md5(content.encode('utf-8')).hexdigest()\n",
    "\n",
    "\n",
    "df[\"content_hash\"] = df[\"text\"].apply(lambda x: hash_content(str(x)) if pd.notna(x) else None)\n",
    "unique_contents = df[[\"text\", \"content_hash\"]].drop_duplicates().dropna()\n",
    "print(f\"Unique content rows: {len(unique_contents)}\", flush=True)\n",
    "\n",
    "chunk_size = 500\n",
    "existing_map = {}\n",
    "\n",
    "for i in range(0, len(unique_contents), chunk_size):\n",
    "    chunk = unique_contents.iloc[i:i + chunk_size]\n",
    "    result = supabase.table(\"dim_content\") \\\n",
    "        .select(\"id\", \"content_hash\") \\\n",
    "        .in_(\"content_hash\", chunk[\"content_hash\"].tolist()) \\\n",
    "        .execute()\n",
    "    if result.data:\n",
    "        existing_map.update({row[\"content_hash\"]: row[\"id\"] for row in result.data})\n",
    "\n",
    "print(f\"Existing content hashes in DB: {len(existing_map)}\", flush=True)\n",
    "\n",
    "\n",
    "missing_rows = unique_contents[~unique_contents[\"content_hash\"].isin(existing_map.keys())]\n",
    "print(f\"Missing content rows to insert: {len(missing_rows)}\", flush=True)\n",
    "\n",
    "if not missing_rows.empty:\n",
    "    to_insert = [\n",
    "        {\"content\": row[\"text\"], \"content_hash\": row[\"content_hash\"]}\n",
    "        for _, row in missing_rows.iterrows()\n",
    "    ]\n",
    "    for i in range(0, len(to_insert), chunk_size):\n",
    "        batch = to_insert[i:i+chunk_size]\n",
    "        inserted = supabase.table(\"dim_content\").insert(batch).execute()\n",
    "        if inserted.data:\n",
    "            for row in inserted.data:\n",
    "                existing_map[row[\"content_hash\"]] = row[\"id\"]\n",
    "\n",
    "\n",
    "content_id_map = {\n",
    "    row[\"text\"]: existing_map[row[\"content_hash\"]]\n",
    "    for _, row in unique_contents.iterrows()\n",
    "    if row[\"content_hash\"] in existing_map\n",
    "}\n",
    "print(f\"Total content IDs mapped: {len(content_id_map)}\", flush=True)\n",
    "dim_cache = {}\n",
    "\n",
    "def get_or_create_id_cached(table, column, value):\n",
    "    key = f\"{table}:{value}\"\n",
    "    if key in dim_cache:\n",
    "        return dim_cache[key]\n",
    "    result = supabase.table(table).select(\"id\").eq(column, value).execute()\n",
    "    if result.data:\n",
    "        dim_cache[key] = result.data[0][\"id\"]\n",
    "        return dim_cache[key]\n",
    "    else:\n",
    "        inserted = supabase.table(table).insert({column: value}).execute()\n",
    "        dim_cache[key] = inserted.data[0][\"id\"]\n",
    "        return dim_cache[key]\n",
    "\n",
    "fact_post_rows = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    try:\n",
    "        if pd.isna(row[\"text\"]) or not str(row[\"text\"]).strip():\n",
    "            continue\n",
    "        subreddit_id = get_or_create_id_cached(\"dim_subreddit\", \"name\", row[\"subreddit\"])\n",
    "        content_id = content_id_map.get(row[\"text\"])\n",
    "        if not content_id:\n",
    "            continue  # skip if content not mapped\n",
    "        year_id = get_or_create_id_cached(\"dim_year\", \"year\", int(row[\"year\"]))\n",
    "        month_id = get_or_create_id_cached(\"dim_month\", \"month\", int(row[\"month\"]))\n",
    "        day = pd.to_datetime(row[\"created_utc\"]).day\n",
    "        day_id = get_or_create_id_cached(\"dim_day\", \"day\", day)\n",
    "        fact_post_rows.append({\n",
    "            \"subreddit_id\": subreddit_id,\n",
    "            \"content_id\": content_id,\n",
    "            \"year_id\": year_id,\n",
    "            \"month_id\": month_id,\n",
    "            \"day_id\": day_id\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping row {idx} due to error: {e}\", flush=True)\n",
    "\n",
    "print(f\"Total fact_post rows ready: {len(fact_post_rows)}\", flush=True)\n",
    "\n",
    "if fact_post_rows:\n",
    "    for i in range(0, len(fact_post_rows), chunk_size):\n",
    "        batch = fact_post_rows[i:i + chunk_size]\n",
    "        supabase.table(\"fact_post\").insert(batch).execute()\n",
    "    print(f\"Inserted {len(fact_post_rows)} rows into fact_post.\", flush=True)\n",
    "else:\n",
    "    print(\"No valid rows to insert.\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10e77d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_summary saved to post_summary.csv\n"
     ]
    }
   ],
   "source": [
    "response = supabase.table(\"post_summary\").select(\"*\").execute()\n",
    "df_summary = pd.DataFrame(response.data)\n",
    "df_summary.to_csv(\"post_summary.csv\", index=False)\n",
    "\n",
    "print(\"post_summary saved to post_summary.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
