{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82067ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "from supabase import create_client, Client\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from itertools import islice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f950fb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3ead679",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"preprocessed_redditData-removedtopic.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "674f1fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started!\n",
      "Unique content rows: 63805\n",
      "Existing content hashes in DB: 0\n",
      "Missing content rows to insert: 63805\n",
      "Total content IDs mapped: 63805\n",
      "Total fact_post rows ready: 63986\n",
      "Inserted 63986 rows into fact_post.\n"
     ]
    }
   ],
   "source": [
    "print(\"Script started!\", flush=True)\n",
    "def hash_content(content: str) -> str:\n",
    "    return hashlib.md5(content.encode('utf-8')).hexdigest()\n",
    "\n",
    "\n",
    "df[\"content_hash\"] = df[\"text\"].apply(lambda x: hash_content(str(x)) if pd.notna(x) else None)\n",
    "unique_contents = df[[\"text\", \"content_hash\"]].drop_duplicates().dropna()\n",
    "print(f\"Unique content rows: {len(unique_contents)}\", flush=True)\n",
    "\n",
    "chunk_size = 500\n",
    "existing_map = {}\n",
    "\n",
    "for i in range(0, len(unique_contents), chunk_size):\n",
    "    chunk = unique_contents.iloc[i:i + chunk_size]\n",
    "    result = supabase.table(\"dim_content\") \\\n",
    "        .select(\"id\", \"content_hash\") \\\n",
    "        .in_(\"content_hash\", chunk[\"content_hash\"].tolist()) \\\n",
    "        .execute()\n",
    "    if result.data:\n",
    "        existing_map.update({row[\"content_hash\"]: row[\"id\"] for row in result.data})\n",
    "\n",
    "print(f\"Existing content hashes in DB: {len(existing_map)}\", flush=True)\n",
    "\n",
    "\n",
    "missing_rows = unique_contents[~unique_contents[\"content_hash\"].isin(existing_map.keys())]\n",
    "print(f\"Missing content rows to insert: {len(missing_rows)}\", flush=True)\n",
    "\n",
    "if not missing_rows.empty:\n",
    "    to_insert = [\n",
    "        {\"content\": row[\"text\"], \"content_hash\": row[\"content_hash\"]}\n",
    "        for _, row in missing_rows.iterrows()\n",
    "    ]\n",
    "    for i in range(0, len(to_insert), chunk_size):\n",
    "        batch = to_insert[i:i+chunk_size]\n",
    "        inserted = supabase.table(\"dim_content\").insert(batch).execute()\n",
    "        if inserted.data:\n",
    "            for row in inserted.data:\n",
    "                existing_map[row[\"content_hash\"]] = row[\"id\"]\n",
    "\n",
    "\n",
    "content_id_map = {\n",
    "    row[\"text\"]: existing_map[row[\"content_hash\"]]\n",
    "    for _, row in unique_contents.iterrows()\n",
    "    if row[\"content_hash\"] in existing_map\n",
    "}\n",
    "print(f\"Total content IDs mapped: {len(content_id_map)}\", flush=True)\n",
    "dim_cache = {}\n",
    "\n",
    "def get_or_create_id_cached(table, column, value):\n",
    "    key = f\"{table}:{value}\"\n",
    "    if key in dim_cache:\n",
    "        return dim_cache[key]\n",
    "    result = supabase.table(table).select(\"id\").eq(column, value).execute()\n",
    "    if result.data:\n",
    "        dim_cache[key] = result.data[0][\"id\"]\n",
    "        return dim_cache[key]\n",
    "    else:\n",
    "        inserted = supabase.table(table).insert({column: value}).execute()\n",
    "        dim_cache[key] = inserted.data[0][\"id\"]\n",
    "        return dim_cache[key]\n",
    "\n",
    "fact_post_rows = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    try:\n",
    "        if pd.isna(row[\"text\"]) or not str(row[\"text\"]).strip():\n",
    "            continue\n",
    "        subreddit_id = get_or_create_id_cached(\"dim_subreddit\", \"name\", row[\"subreddit\"])\n",
    "        content_id = content_id_map.get(row[\"text\"])\n",
    "        if not content_id:\n",
    "            continue  # skip if content not mapped\n",
    "        year_id = get_or_create_id_cached(\"dim_year\", \"year\", int(row[\"year\"]))\n",
    "        month_id = get_or_create_id_cached(\"dim_month\", \"month\", int(row[\"month\"]))\n",
    "        day = pd.to_datetime(row[\"created_utc\"]).day\n",
    "        day_id = get_or_create_id_cached(\"dim_day\", \"day\", day)\n",
    "        fact_post_rows.append({\n",
    "            \"subreddit_id\": subreddit_id,\n",
    "            \"content_id\": content_id,\n",
    "            \"year_id\": year_id,\n",
    "            \"month_id\": month_id,\n",
    "            \"day_id\": day_id\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping row {idx} due to error: {e}\", flush=True)\n",
    "\n",
    "print(f\"Total fact_post rows ready: {len(fact_post_rows)}\", flush=True)\n",
    "\n",
    "if fact_post_rows:\n",
    "    for i in range(0, len(fact_post_rows), chunk_size):\n",
    "        batch = fact_post_rows[i:i + chunk_size]\n",
    "        supabase.table(\"fact_post\").insert(batch).execute()\n",
    "    print(f\"Inserted {len(fact_post_rows)} rows into fact_post.\", flush=True)\n",
    "else:\n",
    "    print(\"No valid rows to insert.\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10e77d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched rows 0 to 999\n",
      "Fetched rows 1000 to 1999\n",
      "Fetched rows 2000 to 2999\n",
      "Fetched rows 3000 to 3999\n",
      "Fetched rows 4000 to 4999\n",
      "Fetched rows 5000 to 5999\n",
      "Fetched rows 6000 to 6999\n",
      "Fetched rows 7000 to 7999\n",
      "Fetched rows 8000 to 8999\n",
      "Fetched rows 9000 to 9999\n",
      "Fetched rows 10000 to 10999\n",
      "Fetched rows 11000 to 11999\n",
      "Fetched rows 12000 to 12999\n",
      "Fetched rows 13000 to 13999\n",
      "Fetched rows 14000 to 14999\n",
      "Fetched rows 15000 to 15999\n",
      "Fetched rows 16000 to 16999\n",
      "Fetched rows 17000 to 17999\n",
      "Fetched rows 18000 to 18999\n",
      "Fetched rows 19000 to 19999\n",
      "Fetched rows 20000 to 20999\n",
      "Fetched rows 21000 to 21999\n",
      "Fetched rows 22000 to 22999\n",
      "Fetched rows 23000 to 23999\n",
      "Fetched rows 24000 to 24999\n",
      "Fetched rows 25000 to 25999\n",
      "Fetched rows 26000 to 26999\n",
      "Fetched rows 27000 to 27999\n",
      "Fetched rows 28000 to 28999\n",
      "Fetched rows 29000 to 29999\n",
      "Fetched rows 30000 to 30999\n",
      "Fetched rows 31000 to 31999\n",
      "Fetched rows 32000 to 32999\n",
      "Fetched rows 33000 to 33999\n",
      "Fetched rows 34000 to 34999\n",
      "Fetched rows 35000 to 35999\n",
      "Fetched rows 36000 to 36999\n",
      "Fetched rows 37000 to 37999\n",
      "Fetched rows 38000 to 38999\n",
      "Fetched rows 39000 to 39999\n",
      "Fetched rows 40000 to 40999\n",
      "Fetched rows 41000 to 41999\n",
      "Fetched rows 42000 to 42999\n",
      "Fetched rows 43000 to 43999\n",
      "Fetched rows 44000 to 44999\n",
      "Fetched rows 45000 to 45999\n",
      "Fetched rows 46000 to 46999\n",
      "Fetched rows 47000 to 47999\n",
      "Fetched rows 48000 to 48999\n",
      "Fetched rows 49000 to 49999\n",
      "Fetched rows 50000 to 50999\n",
      "Fetched rows 51000 to 51999\n",
      "Fetched rows 52000 to 52999\n",
      "Fetched rows 53000 to 53999\n",
      "Fetched rows 54000 to 54999\n",
      "Fetched rows 55000 to 55999\n",
      "Fetched rows 56000 to 56999\n",
      "Fetched rows 57000 to 57999\n",
      "Fetched rows 58000 to 58999\n",
      "Fetched rows 59000 to 59999\n",
      "Fetched rows 60000 to 60999\n",
      "Fetched rows 61000 to 61999\n",
      "Fetched rows 62000 to 62999\n",
      "Fetched rows 63000 to 63999\n",
      "All data saved to post_summary.csv\n"
     ]
    }
   ],
   "source": [
    "all_data = []\n",
    "batch_size = 1000\n",
    "start = 0\n",
    "\n",
    "while True:\n",
    "    end = start + batch_size - 1\n",
    "    response = supabase.table(\"post_summary\").select(\"*\").range(start, end).execute()\n",
    "\n",
    "    if not response.data:\n",
    "        break\n",
    "\n",
    "    all_data.extend(response.data)\n",
    "    print(f\"Fetched rows {start} to {end}\")\n",
    "    start += batch_size\n",
    "\n",
    "df_summary = pd.DataFrame(all_data)\n",
    "df_summary.to_csv(\"post_summary.csv\", index=False)\n",
    "print(\"All data saved to post_summary.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
