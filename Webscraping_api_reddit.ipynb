{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb5ba38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "    \n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5286af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "DEVICE = 0 if torch.cuda.is_available() else -1\n",
    "YEARS = list(range(2020, 2025))\n",
    "SUBREDDITS = [\n",
    "    'dota2', 'valorant', 'leagueoflegends', 'overwatch', 'fortnite',\n",
    "    'Genshin_Impact', 'assasinscreed', 'amongus', 'minecraft', 'monsterhunter'\n",
    "]\n",
    "POST_LIMIT = 100\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f350f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load HuggingFace Pipelines (GPU-enabled)\n",
    "twitter_roberta_sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment\",\n",
    "    tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment\",\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "emotion_pipeline = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
    "    tokenizer=\"j-hartmann/emotion-english-distilroberta-base\",\n",
    "    top_k=1,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f193c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit auth\n",
    "reddit = praw.Reddit(\n",
    "    client_id='q_dYyqYYdNNInGsM-lC9Xg',\n",
    "    client_secret='pLigWA6vX6llH7NjWBhVWmg-gJjKvg',\n",
    "    user_agent='script:gaming_trend (by /u/HiGhastlyy)',\n",
    "    username='HiGhastlyy',\n",
    "    password='0306terror'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc861d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def get_year(utc_timestamp):\n",
    "    return datetime.fromtimestamp(utc_timestamp).year\n",
    "\n",
    "def batch_analyze(texts):\n",
    "    results = []\n",
    "    sentiments = twitter_roberta_sentiment_pipeline(texts, truncation=True, max_length=512)\n",
    "    emotions = emotion_pipeline(texts, truncation=True, max_length=512)\n",
    "    \n",
    "    for sent, emo in zip(sentiments, emotions):\n",
    "        results.append({\n",
    "            'sentiment': sent['label'],\n",
    "            'sentiment_score': sent['score'],\n",
    "            'emotion': emo[0]['label'],\n",
    "            'emotion_score': emo[0]['score']\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affea11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Scraping r/dota2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Posts in r/dota2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:29<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 284 rows to subreddit_outputs\\subreddit_dota2.csv\n",
      "\n",
      "ðŸš€ Scraping r/valorant...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Posts in r/valorant: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [04:53<00:00,  2.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 1033 rows to subreddit_outputs\\subreddit_valorant.csv\n",
      "\n",
      "ðŸš€ Scraping r/leagueoflegends...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Posts in r/leagueoflegends: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [03:50<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 658 rows to subreddit_outputs\\subreddit_leagueoflegends.csv\n",
      "\n",
      "ðŸš€ Scraping r/overwatch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Posts in r/overwatch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:26<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 243 rows to subreddit_outputs\\subreddit_overwatch.csv\n",
      "\n",
      "ðŸš€ Scraping r/fortnite...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Posts in r/fortnite: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:06<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 367 rows to subreddit_outputs\\subreddit_fortnite.csv\n",
      "\n",
      "ðŸš€ Scraping r/Genshin_Impact...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Posts in r/Genshin_Impact: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [05:33<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 1039 rows to subreddit_outputs\\subreddit_Genshin_Impact.csv\n",
      "\n",
      "ðŸš€ Scraping r/assasinscreed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Posts in r/assasinscreed: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:15<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 94 rows to subreddit_outputs\\subreddit_assasinscreed.csv\n",
      "\n",
      "ðŸš€ Scraping r/amongus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Posts in r/amongus: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [05:59<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 1025 rows to subreddit_outputs\\subreddit_amongus.csv\n",
      "\n",
      "ðŸš€ Scraping r/minecraft...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Posts in r/minecraft: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [05:12<00:00,  3.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 892 rows to subreddit_outputs\\subreddit_minecraft.csv\n",
      "\n",
      "ðŸš€ Scraping r/monsterhunter...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Posts in r/monsterhunter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:38<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 376 rows to subreddit_outputs\\subreddit_monsterhunter.csv\n"
     ]
    }
   ],
   "source": [
    "# Scraping\n",
    "all_data = []\n",
    "\n",
    "output_dir = \"subreddit_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for sub_name in SUBREDDITS:\n",
    "    output_path = os.path.join(output_dir, f\"subreddit_{sub_name}.csv\")\n",
    "    \n",
    "    # Skip if already processed\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"Skipping r/{sub_name} (already scraped)\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nScraping r/{sub_name}...\")\n",
    "    subreddit = reddit.subreddit(sub_name)\n",
    "    posts = list(subreddit.top(limit=POST_LIMIT, time_filter='all'))\n",
    "\n",
    "    all_data = []\n",
    "    post_batch_texts = []\n",
    "    post_batch_meta = []\n",
    "    comment_batch_texts = []\n",
    "    comment_batch_meta = []\n",
    "\n",
    "    for post in tqdm(posts, desc=f\"Posts in r/{sub_name}\"):\n",
    "        post_year = get_year(post.created_utc)\n",
    "        if post_year not in YEARS:\n",
    "            continue\n",
    "\n",
    "        post_text = f\"{post.title} {post.selftext}\".strip()\n",
    "        if len(post_text) >= 10:\n",
    "            post_batch_texts.append(post_text)\n",
    "            post_batch_meta.append({\n",
    "                'type': 'post',\n",
    "                'subreddit': sub_name,\n",
    "                'year': post_year,\n",
    "                'id': post.id,\n",
    "                'parent_id': None,\n",
    "                'text': post_text,\n",
    "                'created_utc': datetime.fromtimestamp(post.created_utc).isoformat()\n",
    "            })\n",
    "\n",
    "        # Comments\n",
    "        post.comments.replace_more(limit=0)\n",
    "        for comment in post.comments[:10]:\n",
    "            comment_text = comment.body.strip()\n",
    "            if len(comment_text) < 10:\n",
    "                continue\n",
    "            comment_year = get_year(comment.created_utc)\n",
    "            if comment_year not in YEARS:\n",
    "                continue\n",
    "            comment_batch_texts.append(comment_text)\n",
    "            comment_batch_meta.append({\n",
    "                'type': 'comment',\n",
    "                'subreddit': sub_name,\n",
    "                'year': comment_year,\n",
    "                'id': comment.id,\n",
    "                'parent_id': post.id,\n",
    "                'text': comment_text,\n",
    "                'created_utc': datetime.fromtimestamp(comment.created_utc).isoformat()\n",
    "            })\n",
    "\n",
    "        # Analyze batched posts\n",
    "        if len(post_batch_texts) >= BATCH_SIZE:\n",
    "            results = batch_analyze(post_batch_texts)\n",
    "            for meta, res in zip(post_batch_meta, results):\n",
    "                all_data.append({**meta, **res})\n",
    "            post_batch_texts.clear()\n",
    "            post_batch_meta.clear()\n",
    "\n",
    "        if len(comment_batch_texts) >= BATCH_SIZE:\n",
    "            results = batch_analyze(comment_batch_texts)\n",
    "            for meta, res in zip(comment_batch_meta, results):\n",
    "                all_data.append({**meta, **res})\n",
    "            comment_batch_texts.clear()\n",
    "            comment_batch_meta.clear()\n",
    "\n",
    "    # Final batch\n",
    "    if post_batch_texts:\n",
    "        results = batch_analyze(post_batch_texts)\n",
    "        for meta, res in zip(post_batch_meta, results):\n",
    "            all_data.append({**meta, **res})\n",
    "    if comment_batch_texts:\n",
    "        results = batch_analyze(comment_batch_texts)\n",
    "        for meta, res in zip(comment_batch_meta, results):\n",
    "            all_data.append({**meta, **res})\n",
    "\n",
    "    # Save this subredditâ€™s data\n",
    "    df = pd.DataFrame(all_data)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved {len(df)} rows to {output_path}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
