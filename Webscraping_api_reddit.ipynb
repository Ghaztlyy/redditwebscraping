{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb5ba38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\School Files\\ITS132L\\redditwebscraping\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import os\n",
    "    \n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5286af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "DEVICE = 0 if torch.cuda.is_available() else -1\n",
    "YEARS = list(range(2020, 2025))\n",
    "SUBREDDITS = [\n",
    "    'dota2', 'valorant', 'leagueoflegends', 'overwatch', 'fortnite',\n",
    "    'Genshin_Impact', 'assasinscreed', 'amongus', 'minecraft', 'monsterhunter'\n",
    "]\n",
    "POST_LIMIT = 200\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08f350f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load HuggingFace Pipelines (GPU-enabled)\n",
    "twitter_roberta_sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment\",\n",
    "    tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment\",\n",
    "    device=DEVICE\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f193c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit auth\n",
    "reddit = praw.Reddit(\n",
    "    client_id='q_dYyqYYdNNInGsM-lC9Xg',\n",
    "    client_secret='pLigWA6vX6llH7NjWBhVWmg-gJjKvg',\n",
    "    user_agent='script:gaming_trend (by /u/HiGhastlyy)',\n",
    "    username='HiGhastlyy',\n",
    "    password='0306terror'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bc861d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def get_year(utc_timestamp):\n",
    "    return datetime.fromtimestamp(utc_timestamp).year\n",
    "\n",
    "def batch_analyze(texts):\n",
    "    results = []\n",
    "    sentiments = twitter_roberta_sentiment_pipeline(texts, truncation=True, max_length=512)\n",
    "   \n",
    "    \n",
    "    for sent in sentiments:\n",
    "        results.append({\n",
    "            'sentiment': sent['label'],\n",
    "            'sentiment_score': sent['score'],\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "affea11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping r/dota2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Posts in r/dota2: 100%|██████████| 200/200 [00:00<00:00, 416928.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 61 rows to subreddit_outputs\\subreddit_dota2.csv\n",
      "\n",
      "Scraping r/valorant...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Posts in r/valorant: 100%|██████████| 200/200 [00:00<00:00, 289661.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 200 rows to subreddit_outputs\\subreddit_valorant.csv\n",
      "\n",
      "Scraping r/leagueoflegends...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Posts in r/leagueoflegends: 100%|██████████| 200/200 [00:00<00:00, 342952.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 144 rows to subreddit_outputs\\subreddit_leagueoflegends.csv\n",
      "\n",
      "Scraping r/overwatch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Posts in r/overwatch: 100%|██████████| 200/200 [00:00<00:00, 558867.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 48 rows to subreddit_outputs\\subreddit_overwatch.csv\n",
      "\n",
      "Scraping r/fortnite...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Posts in r/fortnite: 100%|██████████| 200/200 [00:00<00:00, 480722.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 69 rows to subreddit_outputs\\subreddit_fortnite.csv\n",
      "\n",
      "Scraping r/Genshin_Impact...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Posts in r/Genshin_Impact: 100%|██████████| 200/200 [00:00<00:00, 323759.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 186 rows to subreddit_outputs\\subreddit_Genshin_Impact.csv\n",
      "\n",
      "Scraping r/assasinscreed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Posts in r/assasinscreed: 100%|██████████| 200/200 [00:00<00:00, 652809.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 19 rows to subreddit_outputs\\subreddit_assasinscreed.csv\n",
      "\n",
      "Scraping r/amongus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Posts in r/amongus: 100%|██████████| 200/200 [00:00<00:00, 302183.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 187 rows to subreddit_outputs\\subreddit_amongus.csv\n",
      "\n",
      "Scraping r/minecraft...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Posts in r/minecraft: 100%|██████████| 200/200 [00:00<00:00, 173497.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 165 rows to subreddit_outputs\\subreddit_minecraft.csv\n",
      "\n",
      "Scraping r/monsterhunter...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Posts in r/monsterhunter: 100%|██████████| 200/200 [00:00<00:00, 355901.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 73 rows to subreddit_outputs\\subreddit_monsterhunter.csv\n"
     ]
    }
   ],
   "source": [
    "# Scraping\n",
    "all_data = []\n",
    "\n",
    "output_dir = \"subreddit_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for sub_name in SUBREDDITS:\n",
    "    output_path = os.path.join(output_dir, f\"subreddit_{sub_name}.csv\")\n",
    "    \n",
    "    # Skip if already processed\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"Skipping r/{sub_name} (already scraped)\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nScraping r/{sub_name}...\")\n",
    "    subreddit = reddit.subreddit(sub_name)\n",
    "    posts = list(subreddit.top(limit=POST_LIMIT, time_filter='all'))\n",
    "\n",
    "    all_data = []\n",
    "    post_batch_texts = []\n",
    "    post_batch_meta = []\n",
    "    \n",
    "\n",
    "    for post in tqdm(posts, desc=f\"Posts in r/{sub_name}\"):\n",
    "        post_year = get_year(post.created_utc)\n",
    "        if post_year not in YEARS:\n",
    "            continue\n",
    "\n",
    "        post_text = f\"{post.title} {post.selftext}\".strip()\n",
    "        if len(post_text) >= 10:\n",
    "            post_batch_texts.append(post_text)\n",
    "            post_batch_meta.append({\n",
    "                'type': 'post',\n",
    "                'subreddit': sub_name,\n",
    "                'year': post_year,\n",
    "                'id': post.id,\n",
    "                'text': post_text,\n",
    "                'created_utc': datetime.fromtimestamp(post.created_utc).isoformat()\n",
    "            })\n",
    "\n",
    "       \n",
    "\n",
    "    # Final batch\n",
    "    if post_batch_texts:\n",
    "        results = batch_analyze(post_batch_texts)\n",
    "        for meta, res in zip(post_batch_meta, results):\n",
    "            all_data.append({**meta, **res})\n",
    "\n",
    "\n",
    "    # Save this subreddit’s data\n",
    "    df = pd.DataFrame(all_data)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved {len(df)} rows to {output_path}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
